# -*- coding: utf-8 -*-
"""
Wan 4-Frame Reference Node - Ultimate Version
- Adjustable gray level for placeholder
- Perfect mask-latent alignment algorithm
- No noise (removed - causes artifacts)
"""

import torch
import node_helpers
import comfy
import comfy.utils
import comfy.clip_vision
from nodes import MAX_RESOLUTION


class WanFourFrameReferenceUltimate:
    """
    4-frame reference node with perfect alignment.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
                "vae": ("VAE",),
                "width": ("INT", {"default": 832, "min": 16, "max": MAX_RESOLUTION, "step": 16}),
                "height": ("INT", {"default": 480, "min": 16, "max": MAX_RESOLUTION, "step": 16}),
                "length": ("INT", {"default": 81, "min": 1, "max": MAX_RESOLUTION, "step": 4}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 4096}),
                
                # ğŸ¨ å ä½é¢œè‰² (ä¿ç•™å¯è°ƒèŠ‚)
                "placeholder_gray_level": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "slider",
                    "tooltip": "Gray level for placeholder. 0.0=black, 0.5=gray, 1.0=white"
                }),
            },
            "optional": {
                "frame_1_image": ("IMAGE",),
                "frame_2_image": ("IMAGE",),
                "frame_2_ratio": ("FLOAT", {
                    "default": 0.33, 
                    "min": 0.0, 
                    "max": 1.0, 
                    "step": 0.01,
                    "display": "slider",
                    "tooltip": "Frame 2 position (0.0-1.0)"
                }),
                "frame_2_strength": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "slider",
                    "tooltip": "Frame 2 strength. 1.0=fully fixed"
                }),
                "enable_frame_2": (["disable", "enable"], {"default": "enable"}),
                
                "frame_3_image": ("IMAGE",),
                "frame_3_ratio": ("FLOAT", {
                    "default": 0.67, 
                    "min": 0.0, 
                    "max": 1.0, 
                    "step": 0.01,
                    "display": "slider",
                    "tooltip": "Frame 3 position (0.0-1.0)"
                }),
                "frame_3_strength": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "slider",
                    "tooltip": "Frame 3 strength. 1.0=fully fixed"
                }),
                "enable_frame_3": (["disable", "enable"], {"default": "enable"}),
                
                "frame_4_image": ("IMAGE",),
                "end_frame_offset": ("INT", {
                    "default": 16,
                    "min": 0,
                    "max": 64,
                    "step": 4,
                    "tooltip": "Move end frame forward by N frames"
                }),
                
                "clip_vision_frame_1": ("CLIP_VISION_OUTPUT",),
                "clip_vision_frame_2": ("CLIP_VISION_OUTPUT",),
                "clip_vision_frame_3": ("CLIP_VISION_OUTPUT",),
                "clip_vision_frame_4": ("CLIP_VISION_OUTPUT",),
            },
        }

    RETURN_TYPES = ("CONDITIONING", "CONDITIONING", "LATENT")
    RETURN_NAMES = ("positive", "negative", "latent")
    FUNCTION = "generate"
    CATEGORY = "ComfyUI-Wan22FMLF/video"
    DESCRIPTION = "4-frame reference with perfect alignment (no noise)"

    def generate(self, positive, negative, vae, width, height, length, batch_size,
                 placeholder_gray_level=0.5,
                 frame_1_image=None, 
                 frame_2_image=None, frame_2_ratio=0.33, frame_2_strength=0.5, enable_frame_2="enable",
                 frame_3_image=None, frame_3_ratio=0.67, frame_3_strength=0.5, enable_frame_3="enable",
                 frame_4_image=None, end_frame_offset=16,
                 clip_vision_frame_1=None, clip_vision_frame_2=None, 
                 clip_vision_frame_3=None, clip_vision_frame_4=None):
        
        spacial_scale = vae.spacial_compression_encode()
        latent_channels = vae.latent_channels
        latent_t = ((length - 1) // 4) + 1  # ğŸ”‘ VAEæ—¶é—´å‹ç¼©æ¯” 4:1
        device = comfy.model_management.intermediate_device()
        
        latent = torch.zeros([batch_size, latent_channels, latent_t, 
                             height // spacial_scale, width // spacial_scale], device=device)
        
        print(f"ğŸ“ Video: {length} frames â†’ Latent: {latent_t} frames (4:1 compression)")
        
        # Resize images
        if frame_1_image is not None:
            frame_1_image = comfy.utils.common_upscale(frame_1_image[:1].movedim(-1, 1), width, height, "bilinear", "center").movedim(1, -1)
        if frame_2_image is not None:
            frame_2_image = comfy.utils.common_upscale(frame_2_image[:1].movedim(-1, 1), width, height, "bilinear", "center").movedim(1, -1)
        if frame_3_image is not None:
            frame_3_image = comfy.utils.common_upscale(frame_3_image[:1].movedim(-1, 1), width, height, "bilinear", "center").movedim(1, -1)
        if frame_4_image is not None:
            frame_4_image = comfy.utils.common_upscale(frame_4_image[:1].movedim(-1, 1), width, height, "bilinear", "center").movedim(1, -1)
        
        # ğŸ”‘ åˆ›å»ºtimeline (çº¯ç°è‰²,æ— å™ªå£°)
        image = torch.ones((length, height, width, 3), device=device) * placeholder_gray_level
        mask = torch.ones((1, 1, latent_t * 4, latent.shape[-2], latent.shape[-1]), device=device)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ§® æ™ºèƒ½å¯¹é½ç®—æ³•: ç¡®ä¿pixel frameå’Œlatent frameå¯¹é½
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        def calculate_aligned_position(ratio, total_frames):
            """
            æ ¹æ®ratioè®¡ç®—pixel frameä½ç½®,å¹¶ç¡®ä¿å¯¹é½åˆ°latentè¾¹ç•Œ
            
            åŸç†:
            - Wan VAEæ˜¯4:1å‹ç¼©,æ¯ä¸ªlatentå¸§å¯¹åº”4ä¸ªpixelå¸§
            - è¦è®©å‚è€ƒå¸§ç¨³å®š,å¿…é¡»è®©å®ƒå¯¹é½åˆ°latentå¸§çš„èµ·å§‹ä½ç½®
            - ä¾‹å¦‚: pixel frame 32 â†’ latent frame 8 âœ…
            -       pixel frame 33 â†’ latent frame 8 ä½†maskä¼šé”™ä½! âŒ
            """
            # 1. æ ¹æ®ratioè®¡ç®—æœŸæœ›ä½ç½®
            desired_pixel_idx = int(total_frames * ratio)
            
            # 2. è®¡ç®—å¯¹åº”çš„latentç´¢å¼•
            latent_idx = desired_pixel_idx // 4
            
            # 3. å¯¹é½åˆ°latentè¾¹ç•Œ (latent_idx * 4 å°±æ˜¯å¯¹é½åçš„pixelä½ç½®)
            aligned_pixel_idx = latent_idx * 4
            
            # 4. ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
            aligned_pixel_idx = max(0, min(aligned_pixel_idx, total_frames - 1))
            
            return aligned_pixel_idx, latent_idx
        
        # è®¡ç®—å„å¸§ä½ç½®
        frame_1_idx = 0
        frame_1_latent_idx = 0
        
        frame_2_idx, frame_2_latent_idx = calculate_aligned_position(frame_2_ratio, length)
        frame_3_idx, frame_3_latent_idx = calculate_aligned_position(frame_3_ratio, length)
        
        # å°¾å¸§å¤„ç†:å…ˆè®¡ç®—,ç„¶åç¡®ä¿ä¸å†²çª
        frame_4_idx_raw = length - 1 - end_frame_offset
        frame_4_idx, frame_4_latent_idx = calculate_aligned_position(frame_4_idx_raw / length, length)
        
        # ç¡®ä¿é¡ºåº: frame_1 < frame_2 < frame_3 < frame_4
        if frame_2_idx <= frame_1_idx + 4:
            frame_2_idx = frame_1_idx + 4
            frame_2_latent_idx = frame_2_idx // 4
        
        if frame_3_idx <= frame_2_idx + 4:
            frame_3_idx = frame_2_idx + 4
            frame_3_latent_idx = frame_3_idx // 4
        
        if frame_4_idx <= frame_3_idx + 4:
            frame_4_idx = frame_3_idx + 4
            frame_4_latent_idx = frame_4_idx // 4
        
        print(f"ğŸ”§ Alignment check:")
        print(f"   Frame 1: pixel[{frame_1_idx}] â†’ latent[{frame_1_latent_idx}] âœ…")
        print(f"   Frame 2: pixel[{frame_2_idx}] â†’ latent[{frame_2_latent_idx}] âœ…")
        print(f"   Frame 3: pixel[{frame_3_idx}] â†’ latent[{frame_3_latent_idx}] âœ…")
        print(f"   Frame 4: pixel[{frame_4_idx}] â†’ latent[{frame_4_latent_idx}] âœ…")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ”µ Frame 1: æ”¾ç½®å‚è€ƒå¸§
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if frame_1_image is not None:
            image[:frame_1_image.shape[0]] = frame_1_image
            mask[:, :, :frame_1_image.shape[0] + 3] = 0.0
            print(f"ğŸ“Œ Frame 1 at pixel[{frame_1_idx}]: mask[0:{frame_1_image.shape[0] + 3}]")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸŸ¡ Frame 2
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if frame_2_image is not None and enable_frame_2 == "enable":
            image[frame_2_idx:frame_2_idx + frame_2_image.shape[0]] = frame_2_image
            mask_value = 1.0 - frame_2_strength
            # ğŸ”‘ maskè¦†ç›–èŒƒå›´: ä»frame_2_idxå¼€å§‹çš„4å¸§
            mask[:, :, frame_2_idx:frame_2_idx + 4] = mask_value
            print(f"ğŸ“Œ Frame 2 at pixel[{frame_2_idx}]: latent[{frame_2_latent_idx}], mask[{frame_2_idx}:{frame_2_idx+4}], strength={frame_2_strength:.2f}")
        elif frame_2_image is not None:
            print(f"âšª Frame 2 (DISABLED)")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸŸ  Frame 3
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if frame_3_image is not None and enable_frame_3 == "enable":
            image[frame_3_idx:frame_3_idx + frame_3_image.shape[0]] = frame_3_image
            mask_value = 1.0 - frame_3_strength
            mask[:, :, frame_3_idx:frame_3_idx + 4] = mask_value
            print(f"ğŸ“Œ Frame 3 at pixel[{frame_3_idx}]: latent[{frame_3_latent_idx}], mask[{frame_3_idx}:{frame_3_idx+4}], strength={frame_3_strength:.2f}")
        elif frame_3_image is not None:
            print(f"âšª Frame 3 (DISABLED)")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ”´ Frame 4
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if frame_4_image is not None:
            image[frame_4_idx:frame_4_idx + frame_4_image.shape[0]] = frame_4_image
            mask[:, :, frame_4_idx:frame_4_idx + 4] = 0.0
            free_frames = length - 1 - frame_4_idx
            print(f"ğŸ“Œ Frame 4 at pixel[{frame_4_idx}]: latent[{frame_4_latent_idx}], mask[{frame_4_idx}:{frame_4_idx+4}], {free_frames} frames after")
        
        print(f"ğŸ¨ Placeholder: gray_level={placeholder_gray_level:.2f} (no noise)")
        
        # Encode timeline
        concat_latent_image = vae.encode(image[:, :, :, :3])
        
        # Reshape mask
        mask = mask.view(1, mask.shape[2] // 4, 4, mask.shape[3], mask.shape[4]).transpose(1, 2)
        
        # Set conditioning
        positive = node_helpers.conditioning_set_values(positive, {"concat_latent_image": concat_latent_image, "concat_mask": mask})
        negative = node_helpers.conditioning_set_values(negative, {"concat_latent_image": concat_latent_image, "concat_mask": mask})
        
        # CLIP Vision
        clip_vision_output = self._merge_clip_vision_outputs(clip_vision_frame_1, clip_vision_frame_2, clip_vision_frame_3, clip_vision_frame_4)
        if clip_vision_output is not None:
            positive = node_helpers.conditioning_set_values(positive, {"clip_vision_output": clip_vision_output})
            negative = node_helpers.conditioning_set_values(negative, {"clip_vision_output": clip_vision_output})
        
        return (positive, negative, {"samples": latent})

    def _merge_clip_vision_outputs(self, *outputs):
        valid_outputs = [o for o in outputs if o is not None]
        if not valid_outputs:
            return None
        if len(valid_outputs) == 1:
            return valid_outputs[0]
        all_states = [o.penultimate_hidden_states for o in valid_outputs]
        combined_states = torch.cat(all_states, dim=-2)
        result = comfy.clip_vision.Output()
        result.penultimate_hidden_states = combined_states
        return result


NODE_CLASS_MAPPINGS = {"WanFourFrameReferenceUltimate": WanFourFrameReferenceUltimate}
NODE_DISPLAY_NAME_MAPPINGS = {"WanFourFrameReferenceUltimate": "Wan 4-Frame Reference ğŸ¨"}
